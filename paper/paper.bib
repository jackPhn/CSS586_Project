@Online{choi19:_encod_music_style_trans_autoen,
  author       = {Kristy Choi AND Curtis Hawthorne AND Ian Simon AND Monica
                  Dinculescu AND Jesse Engel},
  title        = {{Encoding Musical Style with Transformer Autoencoders}},
  year         = 2019,
  archiveprefix= {arXiv},
  eprint       = {1912.05537v2},
  primaryclass = {cs.SD}
}

@article{huang_music_2018,
	year = {2018},
	title = {Music Transformer},
	url = {http://arxiv.org/abs/1809.04281},
	abstract = {Music relies heavily on repetition to build structure and meaning. Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with {ABA} structure. The Transformer (Vaswani et al., 2017), a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important. Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance (Shaw et al., 2018). This is impractical for long sequences such as musical compositions since their memory complexity for intermediate relative information is quadratic in the sequence length. We propose an algorithm that reduces their intermediate memory requirement to linear in the sequence length. This enables us to demonstrate that a Transformer with our modified relative attention mechanism can generate minute-long compositions (thousands of steps, four times the length modeled in Oore et al., 2018) with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies. We evaluate the Transformer with our relative attention mechanism on two datasets, {JSB} Chorales and Piano-e-Competition, and obtain state-of-the-art results on the latter.},
	journaltitle = {{arXiv}:1809.04281 [cs, eess, stat]},
	author = {Huang, Cheng-Zhi Anna and Vaswani, Ashish and Uszkoreit, Jakob and Shazeer, Noam and Simon, Ian and Hawthorne, Curtis and Dai, Andrew M. and Hoffman, Matthew D. and Dinculescu, Monica and Eck, Douglas},
	urldate = {2021-04-03},
	date = {2018-12-12},
	eprinttype = {arxiv},
	eprint = {1809.04281},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv.org Snapshot:/home/alex/Zotero/storage/HWIEIEIN/1809.html:text/html;arXiv Fulltext PDF:/home/alex/Zotero/storage/BFAISKEM/Huang et al. - 2018 - Music Transformer.pdf:application/pdf}
}

@article{roche_autoencoders_nodate,
	year = {2019},
	title = {Autoencoders for music sound modeling: a comparison of linear, shallow, deep, recurrent and variational models},
	abstract = {This study investigates the use of non-linear unsupervised dimensionality reduction techniques to compress a music dataset into a low-dimensional representation which can be used in turn for the synthesis of new sounds. We systematically compare (shallow) autoencoders ({AEs}), deep autoencoders ({DAEs}), recurrent autoencoders (with Long {ShortTerm} Memory cells – {LSTM}-{AEs}) and variational autoencoders ({VAEs}) with principal component analysis ({PCA}) for representing the high-resolution short-term magnitude spectrum of a large and dense dataset of music notes into a lower-dimensional vector (and then convert it back to a magnitude spectrum used for sound resynthesis). Our experiments were conducted on the publicly available multiinstrument and multi-pitch database {NSynth}. Interestingly and contrary to the recent literature on image processing, we can show that {PCA} systematically outperforms shallow {AE}. Only deep and recurrent architectures ({DAEs} and {LSTM}-{AEs}) lead to a lower reconstruction error. The optimization criterion in {VAEs} being the sum of the reconstruction error and a regularization term, it naturally leads to a lower reconstruction accuracy than {DAEs} but we show that {VAEs} are still able to outperform {PCA} while providing a low-dimensional latent space with nice “usability” properties. We also provide corresponding objective measures of perceptual audio quality ({PEMO}-Q scores), which generally correlate well with the reconstruction error.},
	pages = {8},
	author = {Roche, Fanny and Hueber, Thomas and Limier, Samuel and Girin, Laurent},
	langid = {english},
	file = {Roche et al. - Autoencoders for music sound modeling a compariso.pdf:/home/alex/Zotero/storage/TVMNHEQ3/Roche et al. - Autoencoders for music sound modeling a compariso.pdf:application/pdf}
}

