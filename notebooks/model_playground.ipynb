{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8712aaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pathlib\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from music21 import *\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from musiclearn import config\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de25768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to Schubert collection\n",
    "schubert_dir = pathlib.Path(config.MUSICNET_MIDI_DIR) / \"Schubert\"\n",
    "\n",
    "# path to final fantasy collection\n",
    "ff_dir = pathlib.Path(config.FF_MIDI_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbe1ed5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of an input sequence\n",
    "SEQUENCE_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3a7f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes(midi_dir):\n",
    "    \"\"\" Get all the notes and chords from the midi files in the collection /ff_midi_songs \"\"\"\n",
    "    notes = []\n",
    "    \n",
    "    noteFile_dir = str(midi_dir) + \"/notes\"\n",
    "    \n",
    "    if not os.path.exists(noteFile_dir):\n",
    "        files_dir = str(midi_dir) + \"/*.mid\"\n",
    "        for file in glob.glob(files_dir):\n",
    "            midi = converter.parse(file)\n",
    "\n",
    "            print(\"Parsing %s\" % file)\n",
    "\n",
    "            notes_to_parse = None\n",
    "\n",
    "            try: # file has instrument parts\n",
    "                s2 = instrument.partitionByInstrument(midi)\n",
    "                notes_to_parse = s2.parts[0].recurse()\n",
    "            except: # file has notes in a flat structure\n",
    "                notes_to_parse = midi.flat.notes\n",
    "\n",
    "            for element in notes_to_parse:\n",
    "                if isinstance(element, note.Note):\n",
    "                     notes.append(str(element.pitch))\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "        # save the features\n",
    "        with open(noteFile_dir, 'wb') as fp:\n",
    "            pickle.dump(notes, fp)\n",
    "    else:\n",
    "        with open(noteFile_dir, 'rb') as fp:\n",
    "            notes = pickle.load(fp)\n",
    "    \n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b75c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the notes\n",
    "notes = get_notes(ff_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18d3b948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. unique notes = 308\n"
     ]
    }
   ],
   "source": [
    "# get the number of unique notes\n",
    "n_vocab = len(set(notes))\n",
    "print(\"No. unique notes =\", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd1f69af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all pitch name\n",
    "pitchNames = sorted(set(item for item in notes))\n",
    "\n",
    "# create a dictionary to map notes to unique integers\n",
    "note_to_int = dict((note, number) for number, note in enumerate(pitchNames))\n",
    "\n",
    "# create a dictionary to map unique integers to notes\n",
    "int_to_note = dict((number, note) for number, note in enumerate(pitchNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c25ea6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(sequence_length, notes, n_vocab, note_to_int):\n",
    "    \"\"\" Prepare the sequences used by the neural network \"\"\"   \n",
    "    network_input = []\n",
    "    network_output = []\n",
    "    \n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i : i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "        \n",
    "    n_samples = len(network_input)\n",
    "    \n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "    network_input = np.reshape(network_input, (n_samples, sequence_length, 1))\n",
    "    # normalize input\n",
    "    network_input = network_input / float(n_vocab)\n",
    "    \n",
    "    network_output = np.array(network_output) #keras.utils.to_categorical(network_output)\n",
    "    \n",
    "    return (network_input, network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79bec718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(input_shape, n_vocab):\n",
    "    \"\"\" create the structure of the neural network \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(\n",
    "        512,\n",
    "        input_shape=(input_shape[1], input_shape[2]),\n",
    "        recurrent_dropout=0.3,\n",
    "        return_sequences=True\n",
    "    ))\n",
    "    model.add(LSTM(512, return_sequences=True, recurrent_dropout=0.3,))\n",
    "    model.add(LSTM(512))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0062cd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_model(sequence_length, notes, n_vocab, note_to_int, epochs = 20, batch_size = 128):\n",
    "    \"\"\" Train a neural network to generate music \"\"\"\n",
    "    # get amount of pitch names\n",
    "    n_vocab = len(set(notes))\n",
    "    \n",
    "    network_input, network_output = prepare_sequences(sequence_length, notes, n_vocab, note_to_int)\n",
    "\n",
    "    model = lstm_model(network_input.shape, n_vocab)\n",
    "    \n",
    "    # train the network\n",
    "    filepath = \"saved_weight.hdf5\"\n",
    "    \n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath,\n",
    "        monitor='loss',\n",
    "        verbose=0,\n",
    "        save_best_only=True,\n",
    "        node='min'\n",
    "    )\n",
    "    \n",
    "    callback_list = [checkpoint_cb]\n",
    "    \n",
    "    model.fit(network_input, network_output, epochs=epochs, batch_size=batch_size, callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27cc0ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100, 512)          1052672   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100, 512)          2099200   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 308)               79156     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 308)               0         \n",
      "=================================================================\n",
      "Total params: 5,464,628\n",
      "Trainable params: 5,463,092\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# test code to output model architecture\n",
    "network_input, network_output = prepare_sequences(SEQUENCE_LENGTH, notes, n_vocab, note_to_int)\n",
    "model = lstm_model(network_input.shape, n_vocab)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7003852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/100\n",
      "350/350 [==============================] - 93s 255ms/step - loss: 5.4432\n",
      "Epoch 2/100\n",
      "350/350 [==============================] - 89s 255ms/step - loss: 4.6746\n",
      "Epoch 3/100\n",
      "350/350 [==============================] - 90s 256ms/step - loss: 4.7543\n",
      "Epoch 4/100\n",
      "350/350 [==============================] - 90s 256ms/step - loss: 4.5641\n",
      "Epoch 5/100\n",
      "350/350 [==============================] - 89s 254ms/step - loss: 4.5015\n",
      "Epoch 6/100\n",
      "350/350 [==============================] - 89s 253ms/step - loss: 4.4817\n",
      "Epoch 7/100\n",
      "350/350 [==============================] - 88s 253ms/step - loss: 4.4565\n",
      "Epoch 8/100\n",
      "350/350 [==============================] - 88s 252ms/step - loss: 4.4354\n",
      "Epoch 9/100\n",
      "350/350 [==============================] - 88s 250ms/step - loss: 4.4322\n",
      "Epoch 10/100\n",
      "350/350 [==============================] - 88s 251ms/step - loss: 4.4062\n",
      "Epoch 11/100\n",
      "350/350 [==============================] - 88s 250ms/step - loss: 4.3768\n",
      "Epoch 12/100\n",
      "350/350 [==============================] - 88s 250ms/step - loss: 4.3407\n",
      "Epoch 13/100\n",
      "350/350 [==============================] - 87s 250ms/step - loss: 4.3227\n",
      "Epoch 14/100\n",
      "350/350 [==============================] - 87s 250ms/step - loss: 4.2737\n",
      "Epoch 15/100\n",
      "350/350 [==============================] - 87s 250ms/step - loss: 4.2425\n",
      "Epoch 16/100\n",
      "350/350 [==============================] - 88s 250ms/step - loss: 4.2437\n",
      "Epoch 17/100\n",
      "350/350 [==============================] - 87s 249ms/step - loss: 4.1824\n",
      "Epoch 18/100\n",
      "350/350 [==============================] - 87s 249ms/step - loss: 4.1447\n",
      "Epoch 19/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 4.1116\n",
      "Epoch 20/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 4.0511\n",
      "Epoch 21/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 4.0148\n",
      "Epoch 22/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 3.9440\n",
      "Epoch 23/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 3.8987\n",
      "Epoch 24/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 3.8227\n",
      "Epoch 25/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 3.7704\n",
      "Epoch 26/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 3.6917\n",
      "Epoch 27/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 3.6156\n",
      "Epoch 28/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 3.5490\n",
      "Epoch 29/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 3.4659\n",
      "Epoch 30/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 3.3890\n",
      "Epoch 31/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 3.3094\n",
      "Epoch 32/100\n",
      "350/350 [==============================] - 87s 247ms/step - loss: 3.2312\n",
      "Epoch 33/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 3.1649\n",
      "Epoch 34/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 3.0967\n",
      "Epoch 35/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 3.0273\n",
      "Epoch 36/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 2.9324\n",
      "Epoch 37/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 2.9004\n",
      "Epoch 38/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 2.8262\n",
      "Epoch 39/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 2.7799\n",
      "Epoch 40/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 2.6847\n",
      "Epoch 41/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 2.5921\n",
      "Epoch 42/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 2.5302\n",
      "Epoch 43/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 2.4572\n",
      "Epoch 44/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 2.4131\n",
      "Epoch 45/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 2.3284\n",
      "Epoch 46/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 2.2528\n",
      "Epoch 47/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 2.1837\n",
      "Epoch 48/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 2.1266\n",
      "Epoch 49/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 2.0615\n",
      "Epoch 50/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 1.9831\n",
      "Epoch 51/100\n",
      "350/350 [==============================] - 87s 249ms/step - loss: 1.9301\n",
      "Epoch 52/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 1.8819\n",
      "Epoch 53/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 1.8111\n",
      "Epoch 54/100\n",
      "350/350 [==============================] - 87s 247ms/step - loss: 1.7409\n",
      "Epoch 55/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 1.6860\n",
      "Epoch 56/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 1.6341\n",
      "Epoch 57/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 1.5681\n",
      "Epoch 58/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 1.5408\n",
      "Epoch 59/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 1.4613\n",
      "Epoch 60/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 1.4171\n",
      "Epoch 61/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 1.3866\n",
      "Epoch 62/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 1.3415\n",
      "Epoch 63/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 1.3107\n",
      "Epoch 64/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 1.2691\n",
      "Epoch 65/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 1.1990\n",
      "Epoch 66/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 1.1474\n",
      "Epoch 67/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 1.1221\n",
      "Epoch 68/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 1.0733\n",
      "Epoch 69/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 1.0295\n",
      "Epoch 70/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 1.0242\n",
      "Epoch 71/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 0.9779\n",
      "Epoch 72/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 0.9237\n",
      "Epoch 73/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 0.8974\n",
      "Epoch 74/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 0.8630\n",
      "Epoch 75/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 0.8369\n",
      "Epoch 76/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 0.8056\n",
      "Epoch 77/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 0.7724\n",
      "Epoch 78/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 0.7464\n",
      "Epoch 79/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 0.7179\n",
      "Epoch 80/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 0.7032\n",
      "Epoch 81/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 0.6860\n",
      "Epoch 82/100\n",
      "350/350 [==============================] - 87s 249ms/step - loss: 0.6603\n",
      "Epoch 83/100\n",
      "350/350 [==============================] - 87s 249ms/step - loss: 0.6319\n",
      "Epoch 84/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 0.6219\n",
      "Epoch 85/100\n",
      "350/350 [==============================] - 87s 249ms/step - loss: 0.5824\n",
      "Epoch 86/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 0.5859\n",
      "Epoch 87/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 0.5651\n",
      "Epoch 88/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 0.5464\n",
      "Epoch 89/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 0.5368\n",
      "Epoch 90/100\n",
      "350/350 [==============================] - 87s 249ms/step - loss: 0.5071\n",
      "Epoch 91/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 0.4991\n",
      "Epoch 92/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 87s 248ms/step - loss: 0.4865\n",
      "Epoch 93/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 0.4615\n",
      "Epoch 94/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 0.4569\n",
      "Epoch 95/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 0.4484\n",
      "Epoch 96/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 0.4386\n",
      "Epoch 97/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 0.4149\n",
      "Epoch 98/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 0.4115\n",
      "Epoch 99/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 0.3956\n",
      "Epoch 100/100\n",
      "350/350 [==============================] - 87s 248ms/step - loss: 0.3989\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "train_lstm_model(SEQUENCE_LENGTH, notes, n_vocab, note_to_int, 100, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cef6c92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences_to_predict(sequence_length, notes, n_vocab, note_to_int):\n",
    "    \"\"\" Prepare the sequences to make prediction \"\"\"\n",
    "    network_input = []\n",
    "    \n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "    \n",
    "    n_samples = len(network_input)\n",
    "    \n",
    "    # reshapre the input\n",
    "    normalized_input = np.reshape(network_input, (n_samples, sequence_length, 1))\n",
    "    # normalize input\n",
    "    normalized_input = normalized_input / float(n_vocab)\n",
    "    \n",
    "    return (network_input, normalized_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b2d21b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lstm_model(input_shape, n_vocab):\n",
    "    \"\"\" Load the saved LSTM model \"\"\"\n",
    "    \n",
    "    model = tf.keras.models.load_model('saved_weight.hdf5')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7540c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_notes(model, network_input, int_to_note, n_vocab):\n",
    "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
    "    # pick a random sequence from the input as a starting point for the prediction\n",
    "    start = np.random.randint(0, len(network_input)-1)\n",
    "\n",
    "    pattern = network_input[start]\n",
    "    prediction_output = []\n",
    "\n",
    "    # generate 500 notes\n",
    "    for note_index in range(500):\n",
    "        prediction_input = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        prediction_input = prediction_input / float(n_vocab)\n",
    "\n",
    "        prediction = model.predict(prediction_input, verbose=0)\n",
    "\n",
    "        index = np.argmax(prediction)\n",
    "        result = int_to_note[index]\n",
    "        prediction_output.append(result)\n",
    "\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:]\n",
    "\n",
    "    return prediction_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30cbb407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_midi(prediction_output, filename):\n",
    "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
    "        from the notes \"\"\"\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        # pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 0.5\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "\n",
    "    midi_stream.write('midi', fp=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4d9ed6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "# generate a new midi file from the train model\n",
    "noteFile_dir = str(ff_dir / \"notes\")\n",
    "    \n",
    "# load the notes used to train the model\n",
    "with open(noteFile_dir, 'rb') as fp:\n",
    "    notes = pickle.load(fp)\n",
    "\n",
    "network_input, normalized_input = prepare_sequences_to_predict(SEQUENCE_LENGTH, notes, n_vocab, note_to_int)\n",
    "    \n",
    "# load saved weights\n",
    "model = load_lstm_model(normalized_input.shape, n_vocab)\n",
    "prediction_output = generate_notes(model, network_input, int_to_note, n_vocab)\n",
    "\n",
    " # make midi file\n",
    "create_midi(prediction_output, \"test_output_ff.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192f4650",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64806e90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdb0ed0c",
   "metadata": {},
   "source": [
    "## WaveNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d8163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d584fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplified_wavenet(n_vocab, sequence_length):\n",
    "    \"\"\" A simplified version of WaveNet without residual and skip connection \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # embedding layer\n",
    "    model.add(Embedding(n_vocab, 100, input_length=sequence_length, trainable=True))\n",
    "    \n",
    "    # causal convolution\n",
    "    model.add(Conv1D(64, 3, padding='causal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPool1D(2))\n",
    "    \n",
    "    # dialated causal convolution\n",
    "    model.add(Conv1D(128,3,activation='relu',dilation_rate=2,padding='causal'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPool1D(2))\n",
    "\n",
    "    model.add(Conv1D(256,3,activation='relu',dilation_rate=4,padding='causal'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPool1D(2))\n",
    "    \n",
    "    #model.add(Conv1D(256,5,activation='relu'))    \n",
    "    model.add(GlobalMaxPool1D())\n",
    "    \n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(n_vocab, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7825a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wavenet_model = simplified_wavenet(n_vocab, SEQUENCE_LENGTH)\n",
    "wavenet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a85d0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences_wavenet(sequence_length, notes, n_vocab, note_to_int):\n",
    "    \"\"\" Prepare the sequences used by the neural network \"\"\"   \n",
    "    network_input = []\n",
    "    network_output = []\n",
    "    \n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i : i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "        \n",
    "    n_samples = len(network_input)\n",
    "    \n",
    "    # no reshape\n",
    "    network_input = np.array(network_input)\n",
    "    # normalize input\n",
    "    network_input = network_input / float(n_vocab)\n",
    "    \n",
    "    network_output = np.array(network_output)\n",
    "    \n",
    "    return (network_input, network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a6e9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the input to the model\n",
    "X, Y = prepare_sequences_wavenet(SEQUENCE_LENGTH, notes, n_vocab, note_to_int)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49f8987",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9765435f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c058db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = ModelCheckpoint(\n",
    "    \"wavenet_weights_2.hdf5\", \n",
    "    monitor='val_loss', \n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    "    )\n",
    "\n",
    "history = wavenet_model.fit(X_train,\n",
    "                    Y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=100,\n",
    "                    validation_data=(X_val, Y_val),\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpoint_cb]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ea6d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model\n",
    "model = tf.keras.models.load_model('wavenet_weights_2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f031ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_notes(model, network_input, int_to_note, n_vocab):\n",
    "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
    "    # pick a random sequence from the input as a starting point for the prediction\n",
    "    start = np.random.randint(0, len(network_input)-1)\n",
    "\n",
    "    pattern = list(network_input[start])\n",
    "    prediction_output = []\n",
    "\n",
    "    # generate 500 notes\n",
    "    for note_index in range(500):\n",
    "        prediction_input = np.reshape(pattern, (1, len(pattern)))\n",
    "        prediction_input = prediction_input / float(n_vocab)\n",
    "\n",
    "        prediction = model.predict(prediction_input, verbose=0)\n",
    "\n",
    "        index = np.argmax(prediction)\n",
    "        result = int_to_note[index]\n",
    "        prediction_output.append(result)\n",
    "\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:]\n",
    "\n",
    "    return prediction_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873a3a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate new sequence with wavenet\n",
    "predicted_notes = generate_notes(wavenet_model, X_val, int_to_note, n_vocab)\n",
    "\n",
    "# convert to MiDi file\n",
    "create_midi(predicted_notes, \"wavenet_sample_output.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf966b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
