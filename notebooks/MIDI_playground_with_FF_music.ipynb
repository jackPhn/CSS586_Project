{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f43165f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pathlib\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, BatchNormalization, Dropout, Dense, Activation\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from musiclearn import config\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "088e5c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_dir = pathlib.Path(config.FF_MIDI_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16bddfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of an input sequence\n",
    "SEQUENCE_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fdbecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes(midi_dir):\n",
    "    \"\"\" Get all the notes and chords from the midi files in the collection /ff_midi_songs \"\"\"\n",
    "    notes = []\n",
    "    \n",
    "    noteFile_dir = str(midi_dir) + \"/notes\"\n",
    "    \n",
    "    if not os.path.exists(noteFile_dir):\n",
    "        files_dir = str(midi_dir) + \"/*.mid\"\n",
    "        for file in glob.glob(files_dir):\n",
    "            midi = converter.parse(file)\n",
    "\n",
    "            print(\"Parsing %s\" % file)\n",
    "\n",
    "            notes_to_parse = None\n",
    "\n",
    "            try: # file has instrument parts\n",
    "                s2 = instrument.partitionByInstrument(midi)\n",
    "                notes_to_parse = s2.parts[0].recurse()\n",
    "            except: # file has notes in a flat structure\n",
    "                notes_to_parse = midi.flat.notes\n",
    "\n",
    "            for element in notes_to_parse:\n",
    "                if isinstance(element, note.Note):\n",
    "                     notes.append(str(element.pitch))\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "        # save the features\n",
    "        with open(noteFile_dir, 'wb') as fp:\n",
    "            pickle.dump(notes, fp)\n",
    "    else:\n",
    "        with open(noteFile_dir, 'rb') as fp:\n",
    "            notes = pickle.load(fp)\n",
    "    \n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e716a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(notes, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the neural network \"\"\"\n",
    "    sequence_length  = 100\n",
    "    \n",
    "    # get all pitch name\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "    \n",
    "    # create a dictionary to map pitches to integers\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "    \n",
    "    network_input = []\n",
    "    network_output = []\n",
    "    \n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i : i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "        \n",
    "    n_samples = len(network_input)\n",
    "    \n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "    network_input = np.reshape(network_input, (n_samples, sequence_length, 1))\n",
    "    # normalize input\n",
    "    network_input = network_input / float(n_vocab)\n",
    "    \n",
    "    network_output = keras.utils.to_categorical(network_output)\n",
    "    \n",
    "    return (network_input, network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bdbd1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(network_input, n_vocab):\n",
    "    \"\"\" create the structure of the neural network \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(\n",
    "        512,\n",
    "        input_shape=(network_input.shape[1], network_input.shape[2]),\n",
    "        recurrent_dropout=0.3,\n",
    "        return_sequences=True\n",
    "    ))\n",
    "    model.add(LSTM(512, return_sequences=True, recurrent_dropout=0.3,))\n",
    "    model.add(LSTM(512))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f455b810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_model(notes, epochs = 20, batch_size = 128):\n",
    "    \"\"\" Train a neural network to generate music \"\"\"\n",
    "    # get amount of pitch names\n",
    "    n_vocab = len(set(notes))\n",
    "    \n",
    "    network_input, network_output = prepare_sequences(notes, n_vocab)\n",
    "\n",
    "    model = lstm_model(network_input, n_vocab)\n",
    "    \n",
    "    # train the network\n",
    "    filepath = \"saved_weight.hdf5\"\n",
    "    \n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath,\n",
    "        monitor='loss',\n",
    "        verbose=0,\n",
    "        save_best_only=True,\n",
    "        node='min'\n",
    "    )\n",
    "    \n",
    "    callback_list = [checkpoint_cb]\n",
    "    \n",
    "    model.fit(network_input, network_output, epochs=epochs, batch_size=batch_size, callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9257833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features\n",
    "notes = get_notes(midi_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40567728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. unique notes = 308\n"
     ]
    }
   ],
   "source": [
    "# get the number of unique notes\n",
    "n_vocab = len(set(notes))\n",
    "print(\"No. unique notes =\", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "802a692b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100, 512)          1052672   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100, 512)          2099200   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 308)               79156     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 308)               0         \n",
      "=================================================================\n",
      "Total params: 5,464,628\n",
      "Trainable params: 5,463,092\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# test code to output model architecture\n",
    "network_input, network_output = prepare_sequences(notes, n_vocab)\n",
    "model = lstm_model(network_input, n_vocab)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de435dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/100\n",
      "350/350 [==============================] - 93s 257ms/step - loss: 5.4220\n",
      "Epoch 2/100\n",
      "350/350 [==============================] - 90s 258ms/step - loss: 4.8532\n",
      "Epoch 3/100\n",
      "350/350 [==============================] - 90s 257ms/step - loss: 4.6725\n",
      "Epoch 4/100\n",
      "350/350 [==============================] - 90s 257ms/step - loss: 4.5484\n",
      "Epoch 5/100\n",
      "350/350 [==============================] - 90s 256ms/step - loss: 4.5128\n",
      "Epoch 6/100\n",
      "350/350 [==============================] - 89s 255ms/step - loss: 4.4742\n",
      "Epoch 7/100\n",
      "350/350 [==============================] - 90s 256ms/step - loss: 4.4683\n",
      "Epoch 8/100\n",
      "350/350 [==============================] - 89s 254ms/step - loss: 4.4536\n",
      "Epoch 9/100\n",
      "350/350 [==============================] - 89s 253ms/step - loss: 4.4326\n",
      "Epoch 10/100\n",
      "350/350 [==============================] - 89s 253ms/step - loss: 4.4185\n",
      "Epoch 11/100\n",
      "350/350 [==============================] - 88s 253ms/step - loss: 4.3983\n",
      "Epoch 12/100\n",
      "350/350 [==============================] - 88s 252ms/step - loss: 4.3919\n",
      "Epoch 13/100\n",
      "350/350 [==============================] - 88s 252ms/step - loss: 4.3548\n",
      "Epoch 14/100\n",
      "350/350 [==============================] - 88s 252ms/step - loss: 4.3460\n",
      "Epoch 15/100\n",
      "350/350 [==============================] - 88s 251ms/step - loss: 4.3331\n",
      "Epoch 16/100\n",
      "350/350 [==============================] - 88s 252ms/step - loss: 4.3198\n",
      "Epoch 17/100\n",
      "350/350 [==============================] - 88s 251ms/step - loss: 4.3004\n",
      "Epoch 18/100\n",
      "350/350 [==============================] - 88s 251ms/step - loss: 4.2972\n",
      "Epoch 19/100\n",
      "350/350 [==============================] - 88s 250ms/step - loss: 4.2844\n",
      "Epoch 20/100\n",
      "350/350 [==============================] - 87s 250ms/step - loss: 4.2681\n",
      "Epoch 21/100\n",
      "350/350 [==============================] - 87s 249ms/step - loss: 4.2400\n",
      "Epoch 22/100\n",
      "350/350 [==============================] - 88s 250ms/step - loss: 4.2199\n",
      "Epoch 23/100\n",
      "350/350 [==============================] - 87s 250ms/step - loss: 4.2044\n",
      "Epoch 24/100\n",
      "350/350 [==============================] - 87s 250ms/step - loss: 4.1989\n",
      "Epoch 25/100\n",
      "350/350 [==============================] - 88s 250ms/step - loss: 4.1866\n",
      "Epoch 26/100\n",
      "350/350 [==============================] - 87s 250ms/step - loss: 4.1647\n",
      "Epoch 27/100\n",
      "350/350 [==============================] - 88s 250ms/step - loss: 4.1422\n",
      "Epoch 28/100\n",
      "350/350 [==============================] - 88s 250ms/step - loss: 4.1175\n",
      "Epoch 29/100\n",
      "350/350 [==============================] - 87s 250ms/step - loss: 4.1008\n",
      "Epoch 30/100\n",
      "350/350 [==============================] - 88s 250ms/step - loss: 4.0830\n",
      "Epoch 31/100\n",
      "350/350 [==============================] - 88s 250ms/step - loss: 4.0744\n",
      "Epoch 32/100\n",
      "350/350 [==============================] - 87s 250ms/step - loss: 4.0710\n",
      "Epoch 33/100\n",
      "350/350 [==============================] - 87s 250ms/step - loss: 4.0436\n",
      "Epoch 34/100\n",
      "350/350 [==============================] - 88s 250ms/step - loss: 4.0025\n",
      "Epoch 35/100\n",
      "350/350 [==============================] - 88s 250ms/step - loss: 4.0018\n",
      "Epoch 36/100\n",
      "350/350 [==============================] - 87s 250ms/step - loss: 3.9690\n",
      "Epoch 37/100\n",
      "350/350 [==============================] - 87s 250ms/step - loss: 3.9431\n",
      "Epoch 38/100\n",
      "350/350 [==============================] - 88s 250ms/step - loss: 3.9155\n",
      "Epoch 39/100\n",
      "350/350 [==============================] - 88s 250ms/step - loss: 3.8898\n",
      "Epoch 40/100\n",
      "350/350 [==============================] - 87s 250ms/step - loss: 3.8570\n",
      "Epoch 41/100\n",
      "350/350 [==============================] - 88s 250ms/step - loss: 3.8226\n",
      "Epoch 42/100\n",
      "350/350 [==============================] - 87s 250ms/step - loss: 3.7766\n",
      "Epoch 43/100\n",
      "350/350 [==============================] - 87s 250ms/step - loss: 3.7624\n",
      "Epoch 44/100\n",
      "350/350 [==============================] - 88s 250ms/step - loss: 3.7074\n",
      "Epoch 45/100\n",
      "350/350 [==============================] - 87s 250ms/step - loss: 3.6906\n",
      "Epoch 46/100\n",
      "350/350 [==============================] - 87s 249ms/step - loss: 3.6421\n",
      "Epoch 47/100\n",
      "350/350 [==============================] - 88s 250ms/step - loss: 3.6076\n",
      "Epoch 48/100\n",
      "350/350 [==============================] - 88s 250ms/step - loss: 3.5402\n",
      "Epoch 49/100\n",
      "350/350 [==============================] - 88s 250ms/step - loss: 3.5194\n",
      "Epoch 50/100\n",
      "350/350 [==============================] - 87s 249ms/step - loss: 3.4730\n",
      "Epoch 51/100\n",
      "350/350 [==============================] - 87s 250ms/step - loss: 3.4356\n",
      "Epoch 52/100\n",
      "350/350 [==============================] - 87s 250ms/step - loss: 3.3737\n",
      "Epoch 53/100\n",
      "350/350 [==============================] - 88s 250ms/step - loss: 3.3192\n",
      "Epoch 54/100\n",
      "350/350 [==============================] - 88s 250ms/step - loss: 3.2758\n",
      "Epoch 55/100\n",
      "350/350 [==============================] - 87s 250ms/step - loss: 3.2125\n",
      "Epoch 56/100\n",
      "350/350 [==============================] - 88s 250ms/step - loss: 3.1576\n",
      "Epoch 57/100\n",
      "350/350 [==============================] - 87s 250ms/step - loss: 3.1075\n",
      "Epoch 58/100\n",
      "350/350 [==============================] - 88s 250ms/step - loss: 3.0614\n",
      "Epoch 59/100\n",
      "350/350 [==============================] - 88s 251ms/step - loss: 3.0204\n",
      "Epoch 60/100\n",
      "350/350 [==============================] - 87s 250ms/step - loss: 2.9768\n",
      "Epoch 61/100\n",
      "350/350 [==============================] - 87s 250ms/step - loss: 2.9123\n",
      "Epoch 62/100\n",
      "350/350 [==============================] - 88s 250ms/step - loss: 2.8720\n",
      "Epoch 63/100\n",
      "350/350 [==============================] - 88s 250ms/step - loss: 2.7972\n",
      "Epoch 64/100\n",
      "350/350 [==============================] - 88s 250ms/step - loss: 2.7298\n",
      "Epoch 65/100\n",
      "350/350 [==============================] - 88s 250ms/step - loss: 2.7021\n",
      "Epoch 66/100\n",
      "350/350 [==============================] - 87s 250ms/step - loss: 2.6225\n",
      "Epoch 67/100\n",
      "350/350 [==============================] - 87s 250ms/step - loss: 2.6204\n",
      "Epoch 68/100\n",
      "350/350 [==============================] - 88s 250ms/step - loss: 2.5137\n",
      "Epoch 69/100\n",
      "350/350 [==============================] - 87s 250ms/step - loss: 2.4455\n",
      "Epoch 70/100\n",
      "350/350 [==============================] - 88s 250ms/step - loss: 2.4027\n",
      "Epoch 71/100\n",
      "350/350 [==============================] - 88s 250ms/step - loss: 2.3437\n",
      "Epoch 72/100\n",
      "214/350 [=================>............] - ETA: 34s - loss: 2.2565"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "train_lstm_model(notes, 100, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af26398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences_to_predict(notes, n_vocab):\n",
    "    \"\"\" Prepare the sequences to make prediction \"\"\"\n",
    "    sequence_length  = 100\n",
    "    \n",
    "    # get all pitch name\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "    \n",
    "    # create a dictionary to map pitches to integers\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "    \n",
    "    network_input = []\n",
    "    \n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "    \n",
    "    n_samples = len(network_input)\n",
    "    \n",
    "    # reshapre the input\n",
    "    normalized_input = np.reshape(network_input, (n_samples, sequence_length, 1))\n",
    "    # normalize input\n",
    "    normalized_input = normalized_input / float(n_vocab)\n",
    "    \n",
    "    return (network_input, normalized_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f0a8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(network_input, n_vocab):\n",
    "    \"\"\" create the structure of the neural network \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(\n",
    "        512,\n",
    "        input_shape=(network_input.shape[1], network_input.shape[2]),\n",
    "        recurrent_dropout=0.3,\n",
    "        return_sequences=True\n",
    "    ))\n",
    "    model.add(LSTM(512, return_sequences=True, recurrent_dropout=0.3,))\n",
    "    model.add(LSTM(512))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    \n",
    "    model.load_weights(\"saved_weight.hdf5\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a1c89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_notes(model, network_input, pitchnames, n_vocab):\n",
    "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
    "    # pick a random sequence from the input as a starting point for the prediction\n",
    "    start = np.random.randint(0, len(network_input)-1)\n",
    "\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    pattern = network_input[start]\n",
    "    prediction_output = []\n",
    "\n",
    "    # generate 500 notes\n",
    "    for note_index in range(500):\n",
    "        prediction_input = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        prediction_input = prediction_input / float(n_vocab)\n",
    "\n",
    "        prediction = model.predict(prediction_input, verbose=0)\n",
    "\n",
    "        index = np.argmax(prediction)\n",
    "        result = int_to_note[index]\n",
    "        prediction_output.append(result)\n",
    "\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "\n",
    "    return prediction_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27834288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_midi(prediction_output):\n",
    "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
    "        from the notes \"\"\"\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        # pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 0.5\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "\n",
    "    midi_stream.write('midi', fp='test_output_ff.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c1cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence():\n",
    "    \"\"\" Generate a midi file \"\"\"\n",
    "    noteFile_dir = str(midi_dir) + \"/notes\"\n",
    "    \n",
    "    # load the notes used to train the model\n",
    "    with open(noteFile_dir, 'rb') as fp:\n",
    "        notes = pickle.load(fp)\n",
    "    \n",
    "    # get all pitch names\n",
    "    pitchnames =  sorted(set(item for item in notes))\n",
    "    # get number of pitches\n",
    "    n_vocab = len(set(notes))\n",
    "    \n",
    "    network_input, normalized_input = prepare_sequences_to_predict(notes, n_vocab)\n",
    "    \n",
    "    # load saved weights\n",
    "    model = load_model(normalized_input, n_vocab)\n",
    "    prediction_output = generate_notes(model, network_input, pitchnames, n_vocab)\n",
    "    # make midi file\n",
    "    create_midi(prediction_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5d1817",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baf392d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931e23ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc990e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e80ede1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08b434a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06b04ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picking one song from MusicNet dataset\n",
    "midi_dir = pathlib.Path(config.MUSICNET_MIDI_DIR)\n",
    "mid_2494 = midi_dir / \"Beethoven\" / \"2494_qt11_1.mid\"\n",
    "#multitrack = pypianoroll.read(mid_2494, resolution=24)\n",
    "\n",
    "midi = converter.parse(mid_2494)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3220b83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9752bfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_to_parse = None\n",
    "notes =[]\n",
    "\n",
    "try: # file has instrument parts\n",
    "    s2 = instrument.partitionByInstrument(midi)\n",
    "    notes_to_parse = s2.parts[0].recurse()\n",
    "except: # file has notes in a flat structure\n",
    "    notes_to_parse = midi.flat.notes\n",
    "\n",
    "for element in notes_to_parse:\n",
    "    if isinstance(element, note.Note):\n",
    "        notes.append(str(element.pitch))\n",
    "    elif isinstance(element, chord.Chord):\n",
    "        notes.append('.'.join(str(n) for n in element.normalOrder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496ed204",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377931af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce5242a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b56119e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88ce94d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8963157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56997a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5d7b97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0354f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61668772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
