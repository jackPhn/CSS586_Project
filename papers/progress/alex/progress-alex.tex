%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf,authorversion]{acmart}
%% NOTE that a single column version may be required for
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to
%% \documentclass[manuscript,screen,review]{acmart}

\begin{document}
\settopmatter{printacmref=false}
\setcopyright{none}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}
\title{Progress Report for CSS 586: \\
Modeling Latent Patterns in Music}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{Alex Kyllo}
\email{akyllo@uw.edu}
\affiliation{%
  \institution{University of Washington}
  \city{Bothell}
  \state{WA}
  \country{USA}
}

\begin{abstract}
This report explores recent research in modeling music with deep learning and
provides a progress report on a project to train a generative model of classical
music on the MusicNet dataset.
\end{abstract}

\keywords{deep learning, neural networks, music}

\maketitle

\section{Introduction}

Machine learning models of music have interesting applications in music
information retrieval and creative tools for musical artists and educators.
Music is complex and challenging to model because it exhibits a hierarchy of
recurring patterns.

Depending on the task, machine learning models of music may be trained on the
audio signal itself, either in a time domain or a frequency domain
representation, or they may be trained on a digital symbolic representation of
music, the most common of which is MIDI (Musical Instrument Digital Interface)
notation. MIDI is an encoding of music as streams of bytes in one or more tracks
or channels, each representing a sequence of 128 possible pitch values, along
with timing, pressure and instrument values. A music transcription model may
convert an audio signal into MIDI, which can easily be converted into other
symbolic representations such as sheet music, while a synthesizer model can
convert MIDI representations into audio signals.

\section{Related Work}

Google's \href{https://magenta.tensorflow.org/}{Magenta} is an umbrella project
for music deep learning research and development of software tools to expose
these models for use by creative artists and students.

MusicVAE is a variational LSTM autoencoder for MIDI that incorporates a
novel hierarchical structure using a ``composer'' recurrent layer in its encoder
model to better capture structure at multiple
levels \cite{roberts_hierarchical_2018}.

Music Transformer is a generative model that borrows its approach from the
Natural Language Processing (NLP) domain, using an attention network to model
MIDI music as a sequence of discrete tokens with relative positional dependencies
\cite{huang_music_2018}.

A major advantage of working with the symbolic representation of music is that
it is of far lower dimensionality than the raw audio waveforms of a recorded
performance, which makes it less computationally expensive. However, there are
many aspects of musical performance that are not captured by a symbolic
representation, so the expressiveness of symbolic generative models is
constrained \cite{manzelli_conditioning_2018}.

Other research has focused on modeling raw audio waveforms directly. WaveNet is
a causal convolutional neural network for generating raw audio waveforms,
developed by Google DeepMind, which achieves state of the art performance in
generating natural sounding speech from text, but is also capable of generating
short, realistic snippets of audio music \cite{oord_wavenet_2016}.

Another model named SampleRNN generates raw audio waveforms using a three-tier
hierarchy of gated recurrent units (GRU) to model recurrent structure at
multiple temporal resolutions \cite{mehri_samplernn_2017}.

Jukebox by OpenAI utilizes a vector-quantized variational autoencoder (VQ-VAE)
to compress raw audio into a sequence of discrete codes and models these
sequences using autoregressive transformers to generate music
\cite{dhariwal2020jukebox}.

Audio data can also be modeled in the frequency domain through the use of
Fourier analysis. The recent MelNet model is trained on spectrograms and can
learn musical structures such as melody and harmony and variations in volume,
timbre and rhythm \cite{vasquez2019melnet}.

A paper from Boston University describes an effort to combine the symbolic and
waveform approaches to music modeling, by training an LSTM to learn melodic
structure of different styles of music, then providing generations from this
model as conditioning inputs to a WaveNet-based raw audio generator
\cite{manzelli_conditioning_2018}.

Prior work points out that the division between symbolic music notes and the sounds of music is analogous to the division between symbolic language and utterances in speech \cite{hawthorne2019enabling}.

\section{Planned Methods}

\section{Progress}

\section{Future Work}

\bibliographystyle{ACM-Reference-Format}
\bibliography{progress-alex}

\end{document}
\endinput
