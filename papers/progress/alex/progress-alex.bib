@inproceedings{roberts_hierarchical_2018,
  title     = {A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music},
  pages     = {10},
  booktitle = {Proceedings of the35th International Conference on Machine Learning},
  author    = {Roberts, Adam and Engel, Jesse and Raffel, Colin and Hawthorne, Curtis and Eck, Douglas},
  date      = {2018},
  langid    = {english},
  file      = {Roberts et al. - A Hierarchical Latent Vector Model for Learning Lo.pdf:C\:\\Users\\jekyllo\\Zotero\\storage\\ZJ2JDLW8\\Roberts et al. - A Hierarchical Latent Vector Model for Learning Lo.pdf:application/pdf}
}
@article{huang_music_2018,
  year         = {2018},
  title        = {Music Transformer},
  url          = {http://arxiv.org/abs/1809.04281},
  abstract     = {Music relies heavily on repetition to build structure and meaning. Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with {ABA} structure. The Transformer (Vaswani et al., 2017), a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important. Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance (Shaw et al., 2018). This is impractical for long sequences such as musical compositions since their memory complexity for intermediate relative information is quadratic in the sequence length. We propose an algorithm that reduces their intermediate memory requirement to linear in the sequence length. This enables us to demonstrate that a Transformer with our modified relative attention mechanism can generate minute-long compositions (thousands of steps, four times the length modeled in Oore et al., 2018) with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies. We evaluate the Transformer with our relative attention mechanism on two datasets, {JSB} Chorales and Piano-e-Competition, and obtain state-of-the-art results on the latter.},
  journaltitle = {{arXiv}:1809.04281 [cs, eess, stat]},
  author       = {Huang, Cheng-Zhi Anna and Vaswani, Ashish and Uszkoreit, Jakob and Shazeer, Noam and Simon, Ian and Hawthorne, Curtis and Dai, Andrew M. and Hoffman, Matthew D. and Dinculescu, Monica and Eck, Douglas},
  urldate      = {2021-04-03},
  date         = {2018-12-12},
  eprinttype   = {arxiv},
  eprint       = {1809.04281},
  keywords     = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
  file         = {arXiv.org Snapshot:/home/alex/Zotero/storage/HWIEIEIN/1809.html:text/html;arXiv Fulltext PDF:/home/alex/Zotero/storage/BFAISKEM/Huang et al. - 2018 - Music Transformer.pdf:application/pdf}
}

@article{oord_wavenet_2016,
	title = {{WaveNet}: A Generative Model for Raw Audio},
	url = {http://arxiv.org/abs/1609.03499},
	shorttitle = {{WaveNet}},
	abstract = {This paper introduces {WaveNet}, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single {WaveNet} can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	journaltitle = {{arXiv}:1609.03499 [cs]},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	urldate = {2021-04-05},
	date = {2016-09-19},
	eprinttype = {arxiv},
	eprint = {1609.03499},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
	file = {arXiv Fulltext PDF:/home/alex/Zotero/storage/WFSA9USD/Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:application/pdf;arXiv.org Snapshot:/home/alex/Zotero/storage/9Q22ZW6B/1609.html:text/html},
}

@article{mehri_samplernn_2017,
	title = {{SampleRNN}: An Unconditional End-to-End Neural Audio Generation Model},
	url = {http://arxiv.org/abs/1612.07837},
	shorttitle = {{SampleRNN}},
	abstract = {In this paper we propose a novel model for unconditional audio generation based on generating one audio sample at a time. We show that our model, which profits from combining memory-less modules, namely autoregressive multilayer perceptrons, and stateful recurrent neural networks in a hierarchical structure is able to capture underlying sources of variations in the temporal sequences over very long time spans, on three datasets of different nature. Human evaluation on the generated samples indicate that our model is preferred over competing models. We also show how each component of the model contributes to the exhibited performance.},
	journaltitle = {{arXiv}:1612.07837 [cs]},
	author = {Mehri, Soroush and Kumar, Kundan and Gulrajani, Ishaan and Kumar, Rithesh and Jain, Shubham and Sotelo, Jose and Courville, Aaron and Bengio, Yoshua},
	urldate = {2021-04-19},
	date = {2017-02-11},
	eprinttype = {arxiv},
	eprint = {1612.07837},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound},
	file = {arXiv Fulltext PDF:/home/alex/Zotero/storage/RVEWG634/Mehri et al. - 2017 - SampleRNN An Unconditional End-to-End Neural Audi.pdf:application/pdf},
}

@misc{vasquez2019melnet,
      title={MelNet: A Generative Model for Audio in the Frequency Domain}, 
      author={Sean Vasquez and Mike Lewis},
      year={2019},
      eprint={1906.01083},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}

@misc{dhariwal2020jukebox,
      title={Jukebox: A Generative Model for Music}, 
      author={Prafulla Dhariwal and Heewoo Jun and Christine Payne and Jong Wook Kim and Alec Radford and Ilya Sutskever},
      year={2020},
      eprint={2005.00341},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}

@article{manzelli_conditioning_2018,
  author    = {Rachel Manzelli and
               Vijay Thakkar and
               Ali Siahkamari and
               Brian Kulis},
  title     = {Conditioning Deep Generative Raw Audio Models for Structured Automatic
               Music},
  journal   = {CoRR},
  volume    = {abs/1806.09905},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.09905},
  archivePrefix = {arXiv},
  eprint    = {1806.09905},
  timestamp = {Mon, 13 Aug 2018 16:47:49 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-09905.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{roche_autoencoders_nodate,
  year     = {2019},
  title    = {Autoencoders for music sound modeling: a comparison of linear, shallow, deep, recurrent and variational models},
  abstract = {This study investigates the use of non-linear unsupervised dimensionality reduction techniques to compress a music dataset into a low-dimensional representation which can be used in turn for the synthesis of new sounds. We systematically compare (shallow) autoencoders ({AEs}), deep autoencoders ({DAEs}), recurrent autoencoders (with Long {ShortTerm} Memory cells – {LSTM}-{AEs}) and variational autoencoders ({VAEs}) with principal component analysis ({PCA}) for representing the high-resolution short-term magnitude spectrum of a large and dense dataset of music notes into a lower-dimensional vector (and then convert it back to a magnitude spectrum used for sound resynthesis). Our experiments were conducted on the publicly available multiinstrument and multi-pitch database {NSynth}. Interestingly and contrary to the recent literature on image processing, we can show that {PCA} systematically outperforms shallow {AE}. Only deep and recurrent architectures ({DAEs} and {LSTM}-{AEs}) lead to a lower reconstruction error. The optimization criterion in {VAEs} being the sum of the reconstruction error and a regularization term, it naturally leads to a lower reconstruction accuracy than {DAEs} but we show that {VAEs} are still able to outperform {PCA} while providing a low-dimensional latent space with nice “usability” properties. We also provide corresponding objective measures of perceptual audio quality ({PEMO}-Q scores), which generally correlate well with the reconstruction error.},
  pages    = {8},
  author   = {Roche, Fanny and Hueber, Thomas and Limier, Samuel and Girin, Laurent},
  langid   = {english},
  file     = {Roche et al. - Autoencoders for music sound modeling a compariso.pdf:/home/alex/Zotero/storage/TVMNHEQ3/Roche et al. - Autoencoders for music sound modeling a compariso.pdf:application/pdf}
}
@online{choi19:_encod_music_style_trans_autoen,
  author        = {Kristy Choi AND Curtis Hawthorne AND Ian Simon AND Monica
                  Dinculescu AND Jesse Engel},
  title         = {{Encoding Musical Style with Transformer Autoencoders}},
  year          = 2019,
  archiveprefix = {arXiv},
  eprint        = {1912.05537v2},
  primaryclass  = {cs.SD}
}

