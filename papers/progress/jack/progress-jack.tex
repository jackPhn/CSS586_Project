%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf,authorversion]{acmart}
%% NOTE that a single column version may be required for
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to
%% \documentclass[manuscript,screen,review]{acmart}

\begin{document}
\settopmatter{printacmref=false}
\setcopyright{none}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}
\title{Progress Report for CSS 586 Project: \\
Music Generation Using Deep Learning}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{Jack Phan}
\email{phan92@uw.edu}
\affiliation{%
  \institution{University of Washington}
  \city{Bothell}
  \state{WA}
  \country{USA}
}

\begin{abstract}

The original goal of the project is music transcription, which is the translation of audio data
to symbolic representation of musical notations. With further consideration, we decided to focus on 
a different problem to better align our work. The new objective is music generation using deep learning.
There are several methods that could be used to generate new music from a corpus of training data: MIDI, 
time-domain sound pressure data, frequency-domain representation of the sound pressure. Regardless of the 
data type, music generation involves the application of a sequence model such as recurrent neural network 
(RNN). The focus of my contribution is using a combination of time-domain and frequency-domain information. 
The current proges includes literature review, data exploration, learning about different representations 
of an audio signal. I am working on building a data pipeline and experiencing with RNN models.
\end{abstract}

\keywords{deep learning, neural networks, music generation, RNN}

\maketitle

\section{Introduction}

Recurrent Neural Network (RNN) has shown its capability in recent years to model time-series sequences. 
Advanced version of RNN such as Gated Recurrent Unit (GRU) and Long Short-Term Memory (LSTM) have been 
successfully used for applications such as voice recognition, text to speech, and other applications 
that involve time-series data. RNN could also be used to generate novel sequences of texts or sounds.

A music recording is a digitized time-varying sequence of pressure displacement. Each point in the 
sequence represents the amplitude of the pressure strength at a moment in time. The sequence could 
also be converted to the frequency-domain representation, which describes its sound compoments in terms 
of amplitude and frequency. A popular representation of a sound segment is spectrogram. The goal of this 
project is generative modeling for short polyphonic music segments. We will build and train deep learning 
models using spectrogram as input, and generate novel music segments using these models.

\section{Related Work}

Musical generative modeling is an interesting area of research. Recent innovations in this domain involve
deep learning using a variety of modelling techniques. This section presents some of the recent works in
this domain.

Historically, the modelling of raw audio data is extremely challenging because a single second of audio recording
could contains up to 44,100 samples in the case of music. Google DeepMind's WaveNet is one of the first models
that successfully learn from raw audio data \cite{oord_wavenet_2016}. It is designed using a technique called 
1D dialated causal convolution that allows an output to capture information from many inputs with a minimal
computational cost. WaveNet was designed for the primary purpose of generating speech which mimics any human voice.
However, since the architecture can be used to model any raw audio signal, it was also used to generate music.
Although the outputs were far from being any masterpieces, the results show the possibility of generating musical
pieces from the raw audio data.




\section{Planned Methods}

\subsection{Datasets}


\subsection{Data Processing}


\subsection{Model Building}

\section{Progress}


\section{Future Work}

\bibliographystyle{ACM-Reference-Format}
\bibliography{progress-jack}

\end{document}
\endinput
