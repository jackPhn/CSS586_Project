@Online{choi19:_encod_music_style_trans_autoen,
  author       = {Kristy Choi AND Curtis Hawthorne AND Ian Simon AND Monica
                  Dinculescu AND Jesse Engel},
  title        = {{Encoding Musical Style with Transformer Autoencoders}},
  year         = 2019,
  archiveprefix= {arXiv},
  eprint       = {1912.05537v2},
  primaryclass = {cs.SD}
}

@article{huang_music_2018,
	year = {2018},
	title = {Music Transformer},
	url = {http://arxiv.org/abs/1809.04281},
	abstract = {Music relies heavily on repetition to build structure and meaning. Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with {ABA} structure. The Transformer (Vaswani et al., 2017), a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important. Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance (Shaw et al., 2018). This is impractical for long sequences such as musical compositions since their memory complexity for intermediate relative information is quadratic in the sequence length. We propose an algorithm that reduces their intermediate memory requirement to linear in the sequence length. This enables us to demonstrate that a Transformer with our modified relative attention mechanism can generate minute-long compositions (thousands of steps, four times the length modeled in Oore et al., 2018) with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies. We evaluate the Transformer with our relative attention mechanism on two datasets, {JSB} Chorales and Piano-e-Competition, and obtain state-of-the-art results on the latter.},
	journaltitle = {{arXiv}:1809.04281 [cs, eess, stat]},
	author = {Huang, Cheng-Zhi Anna and Vaswani, Ashish and Uszkoreit, Jakob and Shazeer, Noam and Simon, Ian and Hawthorne, Curtis and Dai, Andrew M. and Hoffman, Matthew D. and Dinculescu, Monica and Eck, Douglas},
	urldate = {2021-04-03},
	date = {2018-12-12},
	eprinttype = {arxiv},
	eprint = {1809.04281},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv.org Snapshot:/home/alex/Zotero/storage/HWIEIEIN/1809.html:text/html;arXiv Fulltext PDF:/home/alex/Zotero/storage/BFAISKEM/Huang et al. - 2018 - Music Transformer.pdf:application/pdf}
}

@article{roche_autoencoders_nodate,
	year = {2019},
	title = {Autoencoders for music sound modeling: a comparison of linear, shallow, deep, recurrent and variational models},
	abstract = {This study investigates the use of non-linear unsupervised dimensionality reduction techniques to compress a music dataset into a low-dimensional representation which can be used in turn for the synthesis of new sounds. We systematically compare (shallow) autoencoders ({AEs}), deep autoencoders ({DAEs}), recurrent autoencoders (with Long {ShortTerm} Memory cells – {LSTM}-{AEs}) and variational autoencoders ({VAEs}) with principal component analysis ({PCA}) for representing the high-resolution short-term magnitude spectrum of a large and dense dataset of music notes into a lower-dimensional vector (and then convert it back to a magnitude spectrum used for sound resynthesis). Our experiments were conducted on the publicly available multiinstrument and multi-pitch database {NSynth}. Interestingly and contrary to the recent literature on image processing, we can show that {PCA} systematically outperforms shallow {AE}. Only deep and recurrent architectures ({DAEs} and {LSTM}-{AEs}) lead to a lower reconstruction error. The optimization criterion in {VAEs} being the sum of the reconstruction error and a regularization term, it naturally leads to a lower reconstruction accuracy than {DAEs} but we show that {VAEs} are still able to outperform {PCA} while providing a low-dimensional latent space with nice “usability” properties. We also provide corresponding objective measures of perceptual audio quality ({PEMO}-Q scores), which generally correlate well with the reconstruction error.},
	pages = {8},
	author = {Roche, Fanny and Hueber, Thomas and Limier, Samuel and Girin, Laurent},
	langid = {english},
	file = {Roche et al. - Autoencoders for music sound modeling a compariso.pdf:/home/alex/Zotero/storage/TVMNHEQ3/Roche et al. - Autoencoders for music sound modeling a compariso.pdf:application/pdf}
}

@misc{liang2019midisandwich2,
      title={MIDI-Sandwich2: RNN-based Hierarchical Multi-modal Fusion Generation VAE networks for multi-track symbolic music generation},
      author={Xia Liang and Junmin Wu and Jing Cao},
      year={2019},
      eprint={1909.03522},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{roberts_hierarchical_2018,
  title     = {A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music},
  pages     = {10},
  booktitle = {Proceedings of the35th International Conference on Machine Learning},
  author    = {Roberts, Adam and Engel, Jesse and Raffel, Colin and Hawthorne, Curtis and Eck, Douglas},
  year      = {2018},
  langid    = {english},
  file      = {Roberts et al. - A Hierarchical Latent Vector Model for Learning Lo.pdf:C\:\\Users\\jekyllo\\Zotero\\storage\\ZJ2JDLW8\\Roberts et al. - A Hierarchical Latent Vector Model for Learning Lo.pdf:application/pdf}
}

@article{oord_wavenet_2016,
  title        = {{WaveNet}: A Generative Model for Raw Audio},
  url          = {http://arxiv.org/abs/1609.03499},
  shorttitle   = {{WaveNet}},
  abstract     = {This paper introduces {WaveNet}, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single {WaveNet} can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
  journaltitle = {{arXiv}:1609.03499 [cs]},
  author       = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  urldate      = {2021-04-05},
  date         = {2016-09-19},
  year         = {2016},
  eprinttype   = {arxiv},
  eprint       = {1609.03499},
  keywords     = {Computer Science - Machine Learning, Computer Science - Sound},
  file         = {arXiv Fulltext PDF:/home/alex/Zotero/storage/WFSA9USD/Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:application/pdf;arXiv.org Snapshot:/home/alex/Zotero/storage/9Q22ZW6B/1609.html:text/html}
}

@article{mehri_samplernn_2017,
  title        = {{SampleRNN}: An Unconditional End-to-End Neural Audio Generation Model},
  url          = {http://arxiv.org/abs/1612.07837},
  shorttitle   = {{SampleRNN}},
  abstract     = {In this paper we propose a novel model for unconditional audio generation based on generating one audio sample at a time. We show that our model, which profits from combining memory-less modules, namely autoregressive multilayer perceptrons, and stateful recurrent neural networks in a hierarchical structure is able to capture underlying sources of variations in the temporal sequences over very long time spans, on three datasets of different nature. Human evaluation on the generated samples indicate that our model is preferred over competing models. We also show how each component of the model contributes to the exhibited performance.},
  journaltitle = {{arXiv}:1612.07837 [cs]},
  author       = {Mehri, Soroush and Kumar, Kundan and Gulrajani, Ishaan and Kumar, Rithesh and Jain, Shubham and Sotelo, Jose and Courville, Aaron and Bengio, Yoshua},
  urldate      = {2021-04-19},
  date         = {2017-02-11},
  year         = {2017},
  eprinttype   = {arxiv},
  eprint       = {1612.07837},
  keywords     = {Computer Science - Artificial Intelligence, Computer Science - Sound},
  file         = {arXiv Fulltext PDF:/home/alex/Zotero/storage/RVEWG634/Mehri et al. - 2017 - SampleRNN An Unconditional End-to-End Neural Audi.pdf:application/pdf}
}

@misc{vasquez2019melnet,
  title         = {MelNet: A Generative Model for Audio in the Frequency Domain},
  author        = {Sean Vasquez and Mike Lewis},
  year          = {2019},
  eprint        = {1906.01083},
  archiveprefix = {arXiv},
  primaryclass  = {eess.AS}
}

@misc{dhariwal2020jukebox,
  title         = {Jukebox: A Generative Model for Music},
  author        = {Prafulla Dhariwal and Heewoo Jun and Christine Payne and Jong Wook Kim and Alec Radford and Ilya Sutskever},
  year          = {2020},
  eprint        = {2005.00341},
  archiveprefix = {arXiv},
  primaryclass  = {eess.AS}
}

@article{manzelli_conditioning_2018,
  author        = {Rachel Manzelli and
               Vijay Thakkar and
               Ali Siahkamari and
               Brian Kulis},
  title         = {Conditioning Deep Generative Raw Audio Models for Structured Automatic
               Music},
  journal       = {CoRR},
  volume        = {abs/1806.09905},
  year          = {2018},
  url           = {http://arxiv.org/abs/1806.09905},
  archiveprefix = {arXiv},
  eprint        = {1806.09905},
  timestamp     = {Mon, 13 Aug 2018 16:47:49 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1806-09905.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@online{choi19:_encod_music_style_trans_autoen,
  author        = {Kristy Choi AND Curtis Hawthorne AND Ian Simon AND Monica
                  Dinculescu AND Jesse Engel},
  title         = {{Encoding Musical Style with Transformer Autoencoders}},
  year          = {2019},
  archiveprefix = {arXiv},
  eprint        = {1912.05537v2},
  primaryclass  = {cs.SD}
}

@misc{hawthorne2019enabling,
  title         = {Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset},
  author        = {Curtis Hawthorne and Andriy Stasyuk and Adam Roberts and Ian Simon and Cheng-Zhi Anna Huang and Sander Dieleman and Erich Elsen and Jesse Engel and Douglas Eck},
  year          = {2019},
  eprint        = {1810.12247},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SD}
}


@article{yang_evaluation_2020,
  title        = {On the evaluation of generative models in music},
  volume       = {32},
  issn         = {0941-0643, 1433-3058},
  url          = {http://link.springer.com/10.1007/s00521-018-3849-7},
  doi          = {10.1007/s00521-018-3849-7},
  abstract     = {The modeling of artiﬁcial, human-level creativity is becoming more and more achievable. In recent years, neural networks have been successfully applied to diﬀerent tasks such as image and music generation, demonstrating their great potential in realizing computational creativity. The fuzzy deﬁnition of creativity combined with varying goals of the evaluated generative systems, however, make subjective evaluation seem to be the only viable methodology of choice. We review the evaluation of generative music systems and discuss the inherent challenges of their evaluation. Although subjective evaluation should always be the ultimate choice for the evaluation of creative results, researchers unfamiliar with rigorous subjective experiment design and without the necessary resources for the execution of a large-scale experiment face challenges in terms of reliability, validity, and replicability of the results. In numerous studies, this leads to the report of insigniﬁcant and possibly irrelevant results and the lack of comparability with similar and previous generative systems. Therefore, we propose a set of simple musically informed objective metrics enabling an objective and reproducible way of evaluating and comparing the output of music generative systems. We demonstrate the usefulness of the proposed metrics with several experiments on real-world data.},
  pages        = {4773--4784},
  number       = {9},
  journaltitle = {Neural Computing and Applications},
  shortjournal = {Neural Comput \& Applic},
  author       = {Yang, Li-Chia and Lerch, Alexander},
  urldate      = {2021-04-25},
  date         = {2020-05},
  year         = {2020},
  langid       = {english},
  file         = {Yang and Lerch - 2020 - On the evaluation of generative models in music.pdf:/home/alex/Zotero/storage/9WDAQXR3/Yang and Lerch - 2020 - On the evaluation of generative models in music.pdf:application/pdf}
}

@misc{dong2017musegan,
  title         = {MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment},
  author        = {Hao-Wen Dong and Wen-Yi Hsiao and Li-Chia Yang and Yi-Hsuan Yang},
  year          = {2017},
  eprint        = {1709.06298},
  archiveprefix = {arXiv},
  primaryclass  = {eess.AS}
}

@article{oore_this_2018,
  title        = {This Time with Feeling: Learning Expressive Musical Performance},
  url          = {http://arxiv.org/abs/1808.03715},
  shorttitle   = {This Time with Feeling},
  abstract     = {Music generation has generally been focused on either creating scores or interpreting them. We discuss differences between these two problems and propose that, in fact, it may be valuable to work in the space of direct \${\textbackslash}it performance\$ generation: jointly predicting the notes \${\textbackslash}it and\$ \${\textbackslash}it also\$ their expressive timing and dynamics. We consider the significance and qualities of the data set needed for this. Having identified both a problem domain and characteristics of an appropriate data set, we show an {LSTM}-based recurrent network model that subjectively performs quite well on this task. Critically, we provide generated examples. We also include feedback from professional composers and musicians about some of these examples.},
  journaltitle = {{arXiv}:1808.03715 [cs, eess]},
  author       = {Oore, Sageev and Simon, Ian and Dieleman, Sander and Eck, Douglas and Simonyan, Karen},
  urldate      = {2021-04-26},
  date         = {2018-08-10},
  year         = {2018},
  eprinttype   = {arxiv},
  eprint       = {1808.03715},
  keywords     = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
  file         = {arXiv Fulltext PDF:C\:\\Users\\jekyllo\\Zotero\\storage\\IH9TKCUJ\\Oore et al. - 2018 - This Time with Feeling Learning Expressive Musica.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jekyllo\\Zotero\\storage\\SKNKH8PB\\1808.html:text/html}
}

@thesis{raffel_learning-based_2016,
  title       = {Learning-Based Methods for Comparing Sequences, with Applications to Audio-to-{MIDI} Alignment and Matching},
  institution = {Columbia University},
  type        = {phdthesis},
  author      = {Raffel, Colin},
  date        = {2016},
  year        = {2016},
  langid      = {english},
  file        = {Raffel - Learning-Based Methods for Comparing Sequences, wi.pdf:C\:\\Users\\jekyllo\\Zotero\\storage\\C8X3GN4K\\Raffel - Learning-Based Methods for Comparing Sequences, wi.pdf:application/pdf}
}


@misc{thickstun2017learning,
      title={Learning Features of Music from Scratch},
      author={John Thickstun and Zaid Harchaoui and Sham Kakade},
      year={2017},
      eprint={1611.09827},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{dong_pypianoroll_2018,
	title = {Pypianoroll: Open Source Python Package for Handling Multitrack Pianoroll},
	abstract = {The pianoroll representation represents music data as timepitch matrices. Although it has been used widely as a way to visualize music data, it has not been much used in computational modeling of music. To promote its usage, we present in this paper a new, open source Python package called Pypianoroll for handling multitrack pianorolls. The core element of Pypianoroll is a new, lightweight multitrack pianoroll format that contains additional tempo and program information along with the pianoroll matrices. It provides tools for creating, manipulating, storing, analyzing and visualizing multitrack pianorolls in an intuitive way. Code and documentation are available at https: //github.com/salu133445/pypianoroll.},
	pages = {2},
	author = {Dong, Hao-Wen and Hsiao, Wen-Yi},
	langid = {english},
	year = {2018},
	file = {Dong and Hsiao - Pypianoroll Open Source Python Package for Handli.pdf:/home/alex/Zotero/storage/4X5RN6FL/Dong and Hsiao - Pypianoroll Open Source Python Package for Handli.pdf:application/pdf},
}

@article{raffel_pretty_midi_2014,
        year = {2018},
	title = {Intuitive Analysis, Creation and Manipulation of MIDI Data With pretty\_midi},
	author = {Raffel, Colin and Ellis, Daniel P.W.},
	pages = {2},
}
@misc{kumar2019polyphonic,
      title={Polyphonic Music Composition with LSTM Neural Networks and Reinforcement Learning},
      author={Harish Kumar and Balaraman Ravindran},
      year={2019},
      eprint={1902.01973},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}
@article{cuthbert_music21_2010,
	title = {music21: A Toolkit for Computer-Aided Musicology and Symbolic Music Data},
	abstract = {Music21 is an object-oriented toolkit for analyzing, searching, and transforming music in symbolic (scorebased) forms. The modular approach of the project allows musicians and researchers to write simple scripts rapidly and reuse them in other projects. The toolkit aims to provide powerful software tools integrated with sophisticated musical knowledge to both musicians with little programming experience (especially musicologists) and to programmers with only modest music theory skills.},
	pages = {6},
	author = {Cuthbert, Michael Scott and Ariza, Christopher},
	date = {2010},
	langid = {english},
	file = {Cuthbert and Ariza - 2010 - music21 A Toolkit for Computer-Aided Musicology a.pdf:/home/alex/Zotero/storage/6TU6QXT6/Cuthbert and Ariza - 2010 - music21 A Toolkit for Computer-Aided Musicology a.pdf:application/pdf},
}
