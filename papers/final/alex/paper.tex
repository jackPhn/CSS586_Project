%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf,authorversion]{acmart}
\usepackage{pgf}
\usepackage{pgfplots}
\usepackage{url}
%% NOTE that a single column version may be required for
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to
%% \documentclass[manuscript,screen,review]{acmart}
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\begin{document}
\settopmatter{printacmref=false}
\setcopyright{none}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}
\title{Modeling the Latent Space of Symbolic String Quartet Music}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{Alex Kyllo}
\email{akyllo@uw.edu}
\affiliation{%
  \institution{University of Washington}
  \city{Bothell}
  \state{WA}
  \country{USA}
}

\begin{abstract}
  This paper presents the results of a project to apply a recurrent
  variational autoencoder to the task of modeling a latent space for
  generating pieces of multi-instrument symbolic classical music
  through interpolation. The code utilized for this research is available at:
  \url{https://github.com/jackPhn/CSS586_Project/}
\end{abstract}

\keywords{deep learning, recurrent neural networks, sequential models,
  music modeling, latent space modeling, generative modeling}

\maketitle

\section{Introduction}

Machine learning models of music have interesting applications in
music information retrieval and creative tools for musical artists and
educators. Generative models can create accompaniments for music,
blend and transfer styles between clips of music, and even generate
entirely new music. Music is challenging to model because it is
inherently high-dimensional, exhibiting a complex hierarchy of
recurring patterns and long-range temporal dependencies, and because
musical scores have multiple possible digital representations with
distinct advantages and disadvantages.

Depending on the task, machine learning models of music may be trained
on the audio signal of a musical performance, either in a time domain
or a frequency domain representation, or they may be trained on a
digital symbolic representation of music, the most common of which is
MIDI (Musical Instrument Digital Interface) notation. MIDI is an
encoding of music as streams of bytes in one or more tracks or
channels, each representing a sequence of 128 possible pitch values
(where 0 is the lowest pitch and 127 is the highest), along with
timing, pressure and instrument identifier values. A generative
symbolic music model can produce a symbolic score in MIDI format,
which must be played by synthesizer software or by humans to produce
an audio music performance. This project focuses on generative
modeling of symbolic (MIDI) music to compose original musical scores
by blending input scores through continuous latent space interpolation.

A survey of related works in generative modeling of polyphonic music
turned up several models focused on multi-instrument pop/rock music
arrangements or classical piano performances. In contrast, this
project focuses on variational autoencoding and latent space
interpolation of classical string quartets (arrangements for an
ensemble of two violins, one viola and one cello) and is, to the
author's knowledge, the first project to apply this modeling approach
to this type of musical arrangement.

\section{Related Work}

The state of the art in music generation still has a long way to go
before it can consistently generate music scores or performances that
would be enjoyable and popular for humans to listen to, but for this
reason it is an area of significant opportunity, where a number of
recent research projects have shown promising progress.

Google's \href{https://magenta.tensorflow.org/}{Magenta} is an
umbrella project for music deep learning research and development of
software tools to expose these models for use by creative artists and
students.

MusicVAE, part of the Magenta project, is a variational Long
Short-Term Memory (LSTM) autoencoder for MIDI that incorporates a
novel hierarchical structure using a ``conductor'' recurrent layer in
its decoder model to better capture structure at multiple levels and
avoid the problem of ``posterior/mode collapse'' whereby a generative
model learns to ignore its latent codes and rely on autoregression to
generate output sequences \cite{roberts_hierarchical_2018}. This model
is trained on 16-bar paragraphs of music and is capable of generating
new melodies that blend two given melodies together via latent space
interpolation. Among the literature surveyed, MusicVAE's approach is
the most similar to this work; in comparison MusicVAE dedicates
significant effort to devising an architecture that allows the model
to generate very long sequences of notes, which is beneficial for
other generation tasks but not strictly necessary for
interpolation. Our model is significantly simpler and demonstrates
that interpolation works well even on a single bar of music at a time.

Another Magenta model called Music Transformer is a generative model
that borrows its approach from the Natural Language Processing (NLP)
domain, using a self-attention network to model MIDI music as a
sequence of discrete tokens with relative positional dependencies
\cite{huang_music_2018}. The focus of this model is on learning
long-term dependencies in music to produce longer clips of music with
coherent structure. Music Transformer was trained on a dataset of
Piano-e-competition performances \cite{hawthorne2019enabling} and its
generated piano music received favorable qualitative (Likert scale)
ratings from human listeners for its resemblance to human-composed
music \cite{huang_music_2018}. In contrast to MusicVAE and this work,
Music Transformer is capable of generating much longer sequences of
music, but does so via sequence extrapolation rather than
interpolation, so it can continue a given priming melody but does not
blend multiple given melodies together. It also assumes a single
instrument track (piano).

MuseGAN \cite{dong2017musegan} is an application of Generative
Adversarial Networks (GAN) to polyphonic MIDI music generation,
trained on four-bar phrases of a multi-track pianoroll representation
of rock songs from the Lakh Midi Dataset
\cite{raffel_learning-based_2016}. Like MusicVAE, MuseGAN includes a
two-level generator that first samples latent codes at the phrase or
bar level, then generates notes within the bars, to produce
longer-term structural patterns.

A major advantage of working with the symbolic representation of music
is that it is of far lower dimensionality than the raw audio waveforms
of a recorded performance, which makes it less computationally
expensive. However, there are many stylistic aspects of musical
performance that are not captured by a symbolic representation, and
may be specific to a particular performer, so the expressiveness of
symbolic generative models is limited in comparison
\cite{manzelli_conditioning_2018}.

Other research has focused on modeling raw audio waveforms directly. WaveNet is
a causal convolutional neural network for generating raw audio waveforms,
developed by Google DeepMind, which achieves state of the art performance in
generating natural sounding speech from text, but is also capable of generating
short, realistic snippets of audio music \cite{oord_wavenet_2016}.
Another model named SampleRNN generates raw audio waveforms using a three-tier
hierarchy of gated recurrent units (GRU) to model recurrent structure at
multiple temporal resolutions \cite{mehri_samplernn_2017}.

Prior work points out that the division between symbolic music notes
and music performances, is analogous to the division between symbolic
language and utterances in speech, which may inspire ideas for
combining the two approaches \cite{hawthorne2019enabling}. A paper
from Boston University describes an effort to combine the symbolic and
waveform approaches to music modeling, by training an LSTM to learn
melodic structure of different styles of music, then providing
generations from this model as conditioning inputs to a WaveNet-based
raw audio generator \cite{manzelli_conditioning_2018}.

While there may be significant opportunity in future approaches that
combining the two, contemporary research generally treats symbolic and
audio music modeling as separate problem domains and this work does
the same, choosing to focus on symbolic music.

\section{Methods}

\subsection{Datasets}

The dataset used for this research project is MusicNet, which is a
collection of 330 freely licensed European classical music recordings
with aligned MIDI scores \cite{thickstun2017learning}.  The model is
trained on 36 string quartets (four-part arrangements with two
violins, one viola and one cello) by composers Bach, Beethoven,
Dvorak, Haydn, Mozart, and Ravel.

\subsection{Data Preprocessing}

Several choices must be made in how to preprocess binary MIDI files
into training examples for a neural network. There are multiple
open-source Python packages that assist with the process of reading
MIDI files from their binary on-disk representations into Python
objects, such as pretty\_midi \cite{raffel_pretty_midi_2014},
Pypianoroll \cite{dong_pypianoroll_2018} and music21
\cite{cuthbert_music21_2010}.

In order to accommodate polyphonic music, we each MIDI file is
converted into a modified pianoroll representation, an example of
which is visualized in Figure \ref{pianoroll}. While the standard
pianoroll represents each time-step as a multi-hot encoded vector of
note values, because most notes are not being played at most timesteps
there is an extreme class imbalance and sparsity in the matrix. This
is reported in \cite{kumar2019polyphonic} to cause problems in model
fitting and metrics calculation, so they introduce an alternative
``multi-stream'' representation of the pianoroll, which we closely,
(but not exactly) emulate in this work.

In our modified representation, each instrument track is a sequence of
integers from 0-129 representing the MIDI note pitch value being
played at the given time step, where values 0-127 are pitches, 128 is
a rest (silence) and 129 is sustain, indicating that the previously
played pitch is held continuously. MIDI data contains pressure values
that indicate how hard or soft each note is played; this information
is discarded in our preprocessing and all notes are represented with
constant pressure. Because chords (multiple notes being played
simultaneously by the same instrument) are relatively rare in string
quartets, the lowest note of each chord is taken and the rest
discarded, to reduce dimensionality. Because notes at the extreme low
and high end are rare and are out of range of common instruments, the
note pitch values are clipped to a narrower range and ordinal
encoded. For the 36 string quartets found in the MusicNet database,
this results in 66 distinct values, where 0-63 represent notes played
by the violins, viola and cello, 64 is rest and 65 is sustain. Figure
\ref{pitch_dist} depicts the distribution of these pitch values per
instrument. Our software package also includes functions to convert
this representation back into a Music21 Score object, which can then
be written to a MIDI file.

\begin{figure}[htbp]
    \begin{center}
        \scalebox{0.6}{\input{figures/pitch_dist.pgf}}
    \end{center}
    \caption{Distribution of pitch values per instrument in the MusicNet string quartets}
    \label{pitch_dist}
\end{figure}

Because songs are typically at least a few minutes long and of varying
length, it will not be feasible to train with entire songs as
examples, so we will crop songs into phrases of equal numbers of
beats to use as training data.

The result of this preprocessing is that each training example isbe a
2D matrix of shape (time steps x instrument tracks) and stacking the
training examples produces a 3D tensor. Information regarding which
track corresponds to which musical instrument is lost and therefore
must be maintained separately to reconstruct the MIDI
representation. For string quartets this instrument ensemble is always
\texttt{[40, 40, 41, 42]}, which is an array of MIDI instrument codes
representing two violins, one viola and one cello. Tempo information
is also lost from this representation, so the outputs are normalized
to the default tempo of 120 beats per minute.

As the model will produce one output per instrument track, it can
incorporate only a fixed selection of instrument parts, similar to how
MusicVAE models three-part (drum, bass and melody)
\cite{roberts_hierarchical_2018} and MuseGAN models five-part (drum,
bass, guitar, string, piano) arrangements. Therefore this model is
purpose-built for string quartets, which are the most common type of
arrangement found in the MusicNet dataset.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{figures/first_bar.png}
  \caption{Pianoroll visualization of the first measure of
    Beethoven's Serioso String Quartet in F Minor}
  \label{pianoroll}
  \Description{A pianoroll representation of cello, viola and violin
    parts from a Beethoven piece.}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{figures/first_bar_sheet.png}
  \caption{Sheet music of the first measure of Beethoven's Serioso
    String Quartet in F Minor}
  \label{sheet}
  \Description{A measure of sheet music cello, viola, and violin parts
    from Beethoven piece.}
\end{figure}

\subsection{Model Design}

The model is a recurrent variational autoencoder; both Long Short-Term
Memory (LSTM) and Gated Recurrent Unit (GRU) cell types were tested
but this choice did not make a significant difference in the results.

The encoder model utilizes an embedding layer to encode the note pitch
integers as vectors; embedding vector length is a tunable
hyperparameter and we found the best results with it set to 8. Then
two LSTM layers, with a dropout layer in between, convert the
sequences into latent codes, which are modeled as a multidimensional
Gaussian distribution $z$, produced by the encoder output Lambda layer
(Figure \ref{decoder}).

The decoder model samples from the latent space $z$ with the
distribution parameterized by the $\mu$ and $log(\sigma)$ values
learned by the encoder, and then utilizes two LSTM layers with a
dropout layer between them to generate sequences from the latent
codes. Then the network splits into four parallel outputs, one per
instrument track, where a fully connected layer with softmax
activation is used to select the most probable note value for the
the current track at the each time step (Figure \ref{decoder}).

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{figures/encoder.png}
  \caption{A directed graph diagram of the encoder network.}
  \label{encoder}
\end{figure}


\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{figures/decoder.png}
  \caption{A directed graph diagram of the decoder network (only one
    of four parallel output layers shown).}
  \label{decoder}
\end{figure}


The autoencoder reconstruction loss function is sparse categorical
cross-entropy, which is added to the Kullback-Leibler divergence of
the approximate posterior distribution from a multivariate Gaussian
distribution with mean 0 and standard deviation 1, resulting in the
(negative) evidence lower bound (ELBO) loss (Equation \ref{nelbo}),
which is a standard variational autoencoder loss function. This loss
function is applied to each track independently and then summed across
all four tracks with equal weight.

\begin{equation}
  \label{nelbo}
\mathbb{E}[log{p_\theta}(x|z)]-\mathbb{KL}(q_\lambda(z|x)||p(z))
\end{equation}

where $\lambda$ and $\theta$ represent the encoder and decoder
parameters respectively, while $q_\lambda(z|x)$ represents the
encoder, which approximates the true posterior probability $p(z|x)$,
and $p_\theta(x|z)$ represents the decoder, which parameterizes the
likelihood $p(x|z)$ \cite{roberts_hierarchical_2018}.



\subsection{Model Experimentation}

The following hyperparameters were tuned, with tested values listed
and best parameters in \textbf{bold}:

\begin{itemize}
  \tightlist
  \item batch size: \textbf{32}, 64, 96
  \item learning rate: 0.001, \textbf{0.0002}
  \item recurrent layer type: LSTM, GRU (made no difference)
  \item recurrent encoder layer directions: \textbf{unidirectional}, bidirectional
  \item recurrent layer cells: 64, \textbf{128}, 256
  \item latent code dimension: 128, 256, \textbf{512}
  \item embedding dimension: 4, \textbf{8}, 16
  \item dropout rate: 0.2, 0.4, \textbf{0.5}
  \item time steps per beat (quarter note): \textbf{4}, 8
  \item beats per phrase: \textbf{4}, 8
\end{itemize}

Each model was trained for up to 500 epochs with an early stopping
patience of 20 epochs. The model was cross-validated on 10\% of the
training data after each epoch. The best model was selected by a
combination of validation loss and listening to hand-picked sample
reconstructions and interpolations.

Data augmentation was also tested--Oore et al (2018)
\cite{oore_this_2018} suggests augmentation via pitch shifting each
training example up or down by up to six semitones in order to create
additional training examples and reduce overfitting, so we followed
that recommendation and used it to double the size of the training
dataset. As a result, we saw much lower validation losses (compare
Figure \ref{loss} and Figure \ref{loss_augment} and a small but
noticeable improvement in the fidelity of the song reconstructions,
suggesting that this method was effective in reducing overfitting.

\begin{figure}[htbp]
    \begin{center}
        \scalebox{0.5}{\input{figures/loss.pgf}}
    \end{center}
    \caption{Training and validation loss for the best performing model - without data augmentation}
    \label{loss}
\end{figure}

\begin{figure}[htbp]
    \begin{center}
        \scalebox{0.5}{\input{figures/loss_augment.pgf}}
    \end{center}
    \caption{Training and validation loss for the best performing model - with data augmentation}
    \label{loss_augment}
\end{figure}


\section{Results}

The reconstructed music produced by the model sounds reminiscent of
the original inputs, preserving the key signature and general rhythmic
and melodic structure, albeit with noticeable differences at the
individual note level.

A collection of samples of the autoencoded outputs and interpolations,
converted to mp3 format, is presented at the following URL:
\url{https://drive.google.com/drive/folders/1SlQOI_cL452PftO3KmOYO_qglE2RotCX?usp=sharing}.
Each of the 18 directories contains five interpolations between two of
the string quartet pieces in the MusicNet corpus, numbered 0-4, where
0 is the reconstruction of the first piece, 4 is the reconstruction of
the second piece, and 1-3 are linear interpolations between the two
pieces. Each file is cropped to the first 16 measures only.

The interpolations are between the following 18 pairs of pieces

\begin{itemize}
\tightlist
\item \texttt{Haydn/2104\_op64n5\_1.mid} , \texttt{Ravel/2178\_gr\_rqtf2.mid}
\item \texttt{Haydn/2105\_op64n5\_2.mid} ,   \texttt{Ravel/2179\_gr\_rqtf3.mid}
\item \texttt{Haydn/2106\_op64n5\_3.mid} ,   \texttt{Ravel/2177\_gr\_rqtf1.mid}
\item \texttt{Beethoven/2497\_qt11\_4.mid} , \texttt{Mozart/1822\_kv\_421\_1.mid}
\item \texttt{Beethoven/2433\_qt16\_3.mid} , \texttt{Mozart/1859\_kv\_464\_2.mid}
\item \texttt{Beethoven/2368\_qt12\_4.mid} , \texttt{Mozart/1807\_kv\_387\_3.mid}
\item \texttt{Beethoven/2314\_qt15\_2.mid} , \texttt{Mozart/1791\_kv\_465\_4.mid}
\item \texttt{Beethoven/2480\_qt05\_1.mid} , \texttt{Mozart/1792\_kv\_465\_1.mid}
\item \texttt{Beethoven/2481\_qt05\_2.mid} , \texttt{Mozart/1835\_kv\_590\_3.mid}
\item \texttt{Beethoven/2379\_qt08\_4.mid} , \texttt{Mozart/1805\_kv\_387\_1.mid}
\item \texttt{Beethoven/2365\_qt12\_1.mid} , \texttt{Mozart/1793\_kv\_465\_2.mid}
\item \texttt{Beethoven/2562\_qt02\_4.mid} , \texttt{Mozart/1790\_kv\_465\_3.mid}
\item \texttt{Beethoven/2494\_qt11\_1.mid} , \texttt{Mozart/1789\_kv\_465\_2.mid}
\item \texttt{Beethoven/2403\_qt01\_4.mid} , \texttt{Mozart/1788\_kv\_465\_1.mid}
\item \texttt{Beethoven/2376\_qt08\_1.mid} , \texttt{Dvorak/1916\_dvq10m1.mid}
\item \texttt{Beethoven/2384\_qt13\_4.mid} , \texttt{Bach/2242\_vs1\_2.mid}
\item \texttt{Beethoven/2560\_qt02\_2.mid} , \texttt{Beethoven/2621\_qt07\_1.mid}
\item \texttt{Beethoven/2377\_qt08\_2.mid} , \texttt{Beethoven/2381\_qt13\_1.mid}
\end{itemize}

\subsection{Model Evaluation}

Evaluation of generative models is challenging because there is no
equivalent of an accuracy metric like what is used in supervised
learning. For autoencoder models we can measure how accurately the
model can reconstruct its own inputs, but this does not tell us the
quality of the interpolated examples. Generative models are typically
evaluated using a combination of qualitative metrics whereby human
judges rate the quality of the generated examples (essentially a
Turing test), and quantitative metrics that assess the differences in
the parametric distributions of generated and real examples. Yang and
Lerch (2020) proposes a set of metrics informed by music theory, for
probabilistically evaluating how similar the generations are to known
sample distributions of real music \cite{yang_evaluation_2020}. These
metrics include counts, ranges, histograms and transition matrices of
pitches and note lengths, then the Kullback-Leibler divergence and
overlapping area of the probability density functions are used to
compare against known reference distributions per musical genre
\cite{yang_evaluation_2020}. Due to the cost and time requirements
associated with designing a human subjects experiment, we
utilize this quantitative approach to the quality assessment of
generated samples.

\begin{figure}[htbp]
    \begin{center}
        \scalebox{0.6}{\input{figures/source_pitch_count.pgf}}
    \end{center}
    \caption{Distinct pitch counts per sample for source samples.}
    \label{source_pitches}
\end{figure}

\begin{figure}[htbp]
    \begin{center}
        \scalebox{0.6}{\input{figures/generated_pitch_count.pgf}}
    \end{center}
    \caption{Distinct pitch counts per sample for generated samples.}
    \label{generated_pitches}
\end{figure}

\begin{figure}[htbp]
    \begin{center}
        \scalebox{0.6}{\input{figures/pitch_class_histogram.pgf}}
    \end{center}
    \caption{Comparative pitch class histogram for source and generated samples.}
    \label{pch}
\end{figure}

\begin{figure}[htbp]
    \begin{center}
        \scalebox{0.6}{\input{figures/pitch_range_source.pgf}}
    \end{center}
    \caption{Pitch range per sample for source samples.}
    \label{source_ranges}
\end{figure}

\begin{figure}[htbp]
    \begin{center}
        \scalebox{0.6}{\input{figures/pitch_range_generated.pgf}}
    \end{center}
    \caption{Pitch range per sample for generated samples.}
    \label{generated_ranges}
\end{figure}

\begin{figure}[htbp]
    \begin{center}
        \scalebox{0.6}{\input{figures/avg_pitch_interval_source.pgf}}
    \end{center}
    \caption{Average pitch interval per sample for source samples.}
    \label{source_interval}
\end{figure}

\begin{figure}[htbp]
    \begin{center}
        \scalebox{0.6}{\input{figures/avg_pitch_interval_generated.pgf}}
    \end{center}
    \caption{Average pitch interval per sample for generated samples.}
    \label{generated_interval}
\end{figure}


\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{figures/pctm_source.png}
  \caption{Pitch class transition matrix for source samples.}
  \label{pctm_source}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{figures/pctm_generated.png}
  \caption{Pitch class transition matrix for generated samples.}
  \label{pctm_generated}
\end{figure}

\section{Discussion}

\bibliographystyle{ACM-Reference-Format}
\bibliography{paper}

\end{document}
\endinput
